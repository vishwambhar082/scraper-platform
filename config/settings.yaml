# Global platform configuration for scraper-platform-v5.0

app:
  name: scraper-platform-v5.0
  environment: dev         # overridden by env-specific configs
  timezone: "UTC"
  default_source: "alfabeta"

logging:
  level: INFO
  json: false
  log_dir: logs

pcid:
  backend: "hash"          # hash | remote | pgvector (future)
  hash:
    dims: 48
    # index_path: "config/pcid_master.jsonl"  # optional prebuilt index
  remote:
    base_url: ""
    timeout: 5.0
    max_retries: 3
    backoff_base: 0.5

scraping:
  default_engine: selenium     # selenium | playwright | http
  default_rate_limit:
    # Minimum delay (in seconds) between HTTP/browser calls
    min_delay: 0.5

    # Maximum delay (in seconds) between calls
    max_delay: 2.0

    # Optional: global cap per worker (not enforced in code yet,
    # but documented so we can wire later if needed)
    max_rps_hint: 1.0
  session_ttl_hours: 12
  max_retries: 3
  retry_backoff_seconds: 5

outputs:
  base_dir: output
  alfabeta_daily_dir: output/alfabeta/daily

export:
  backends:
    - csv
  s3:
    bucket: ""
    prefix: ""
  gcs:
    bucket: ""
    prefix: ""

database:
  driver: postgres
  host: localhost
  port: 5432
  user: scraper_user
  password: scraper_password
  name: scraper_db

redis:
  enabled: false
  host: localhost
  port: 6379
  db: 0

# Account rotation for sources (used by resource_manager.account_router)
# NOTE: These are sample credentials provided for local use; replace with
# secret-managed values in production (Vault/ENV).
accounts:
  alfabeta:
    acc1: ["vishwambhar080@gmail.com", "iacG@V3hK8LrFUL"]

airflow:
  enabled: false
  home: ./airflow_home

api:
  host: "0.0.0.0"
  port: 8000

# Data retention policies for operational tables. Used by tools/cleanup_db.py.
retention:
  runs_days: 90
  incidents_days: 180
