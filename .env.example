# Scraper Platform Environment Configuration
# Copy this file to .env and update with your actual values

# ============================================================================
# REQUIRED FOR PRODUCTION
# ============================================================================

# Database Configuration
# Use DB_URL for all database connections (single source of truth)
DB_URL=postgresql://scraper:CHANGE_PASSWORD_HERE@postgres:5432/scraperdb

# API Security
# Generate with: python3 -c "import secrets; print(secrets.token_urlsafe(32))"
SCRAPER_SECRET_KEY=CHANGE-ME-TO-A-SECURE-RANDOM-VALUE

# Crypto Key for encrypting sensitive data (cookies, tokens)
# Generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
CRYPTO_KEY=CHANGE-ME-TO-A-FERNET-KEY

# CORS Origins (comma-separated list of allowed origins)
# Required since Phase 1 security fix
CORS_ORIGINS=http://localhost:3000,http://localhost:8000

# ============================================================================
# REQUIRED FOR DOCKER COMPOSE
# ============================================================================

# PostgreSQL (docker-compose postgres service)
# Must match credentials in DB_URL
POSTGRES_USER=scraper
POSTGRES_PASSWORD=CHANGE_PASSWORD_HERE
POSTGRES_DB=scraperdb

# Airflow Configuration
# Generate Fernet key with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW__CORE__FERNET_KEY=CHANGE-ME-TO-A-FERNET-KEY
# Must match PostgreSQL credentials above
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://scraper:CHANGE_PASSWORD_HERE@postgres/scraperdb

# ============================================================================
# OPTIONAL CONFIGURATION
# ============================================================================

# Logging
LOG_LEVEL=INFO
SCRAPER_LOG_LEVEL=INFO

# Local Development - Use SQLite instead of PostgreSQL
# Uncomment to use file-based database for local runs (no Postgres required)
# RUN_DB_PATH=logs/run_tracking.sqlite

# Disable Database Entirely (for testing)
# SCRAPER_PLATFORM_DISABLE_DB=1

# Browser Automation
# Set to 1 to use fake browser for testing (no actual browser required)
# SCRAPER_PLATFORM_FAKE_BROWSER=1

# Python Path (for docker containers)
PYTHONPATH=/app

# ============================================================================
# INTEGRATIONS (Optional)
# ============================================================================

# Slack Notifications
# SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
# SLACK_BOT_TOKEN=xoxb-your-token-here

# JIRA Integration
# JIRA_BASE_URL=https://your-company.atlassian.net
# JIRA_USERNAME=your-email@company.com
# JIRA_API_TOKEN=your-api-token

# Email (SMTP)
# SMTP_HOST=localhost
# SMTP_PORT=587
# SMTP_USERNAME=your-email@company.com
# SMTP_PASSWORD=your-password

# LLM APIs (for agent features)
# OPENAI_API_KEY=sk-your-key-here
# GROQ_API_KEY=gsk_your-key-here

# Secrets Management
# CREDENTIAL_BACKEND=vault  # or "local"
# VAULT_ADDR=https://vault.company.com
# VAULT_TOKEN=your-vault-token

# Observability
# PROMETHEUS_ENABLED=1
# PROMETHEUS_PORT=9100

# ============================================================================
# DEPRECATED - DO NOT USE
# ============================================================================

# The following variables are deprecated and ignored:
# - DB_HOST, DB_PORT, DB_USER, DB_PASSWORD, DB_NAME (use DB_URL instead)
# - FEATURE_FLAGS (not implemented)
# - GX_ENABLED (Great Expectations integration incomplete)
# - SCRAPER_PLATFORM_VERSION (never read)
