# Scraper Platform v5.0 ğŸš€

> **AI-powered web scraping platform with autonomous agent system**

## Overview

The Scraper Platform is a unified pipeline architecture for scalable, maintainable web scraping. It features self-healing scrapers using LLMs for anomaly detection and patch generation, orchestrated by Apache Airflow DAGs.

## Key Features

- **ğŸ¤– Autonomous Agents**: Self-healing scrapers with LLM-powered repair capabilities
- **ğŸ”— Unified Pipeline**: Single execution model replacing multiple orchestrators
- **âš¡ Parallel Execution**: Automatic parallelization of independent steps
- **ğŸ“‹ Type-Safe Steps**: Clear step types (FETCH, PARSE, TRANSFORM, VALIDATE, ENRICH, EXPORT)
- **ğŸ›¡ï¸ Robust Error Handling**: Built-in retry logic with exponential backoff
- **ğŸ“Š Comprehensive Tracking**: Full monitoring and logging of all pipeline steps

## Quick Start

### Prerequisites

- Python 3.11+
- Docker (recommended for Airflow)

### Installation

```bash
# Clone repository
git clone <repository-url>
cd scraper-platform

# Install dependencies
python -m venv .venv
source .venv/bin/activate          # Windows: .\.venv\Scripts\activate
pip install -r requirements.txt

# Set up environment
cp config/env/example.env .env
# Edit .env with your settings

# Create necessary directories (or run scripts/setup_dev.sh)
mkdir -p sessions/cookies sessions/logs output/alfabeta/daily input
```

## Running the Platform

### Option 1: Using Docker (Recommended)

```bash
# Build and start services
docker-compose up -d

# Access Airflow UI at http://localhost:8080
# Default credentials: admin/admin
```

### Option 2: Direct Execution (No Airflow)

```bash
# Run a scraper directly (canonical entrypoint)
python -m src.entrypoints.run_pipeline --source alfabeta --environment dev

# Run with the desktop UI
python run_ui.py
```

## Project Structure

```
scraper-platform/
â”œâ”€â”€ config/                 # Configuration files
â”‚   â”œâ”€â”€ env/               # Environment configs
â”‚   â”œâ”€â”€ sources/           # Source-specific configs
â”‚   â””â”€â”€ ...
â”œâ”€â”€ dags/                  # Airflow DAG definitions
â”œâ”€â”€ db/                    # Database migrations
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ architecture/      # Architecture docs
â”‚   â”œâ”€â”€ guides/            # User guides
â”‚   â”œâ”€â”€ reference/         # Reference materials
â”‚   â”œâ”€â”€ tutorials/         # Tutorials
â”‚   â””â”€â”€ troubleshooting/   # Troubleshooting guides
â”œâ”€â”€ dsl/                   # Pipeline definitions
â”œâ”€â”€ schemas/               # Data schemas
â”œâ”€â”€ src/                   # Source code
â”‚   â”œâ”€â”€ agents/            # Autonomous agents
â”‚   â”œâ”€â”€ api/               # API endpoints
â”‚   â”œâ”€â”€ engines/           # Scraping engines
â”‚   â”œâ”€â”€ pipeline/          # Pipeline system
â”‚   â”œâ”€â”€ scrapers/          # Source-specific scrapers
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tests/                 # Test suite
â””â”€â”€ tools/                 # Utility tools
```

## Available Sources

- Alfabeta
- Argentina
- Chile
- LAFA
- Quebec

## Documentation

- [Architecture Overview](docs/architecture/ARCHITECTURE_V5.md)
- [Quick Start Guide](docs/tutorials/QUICK_START_V5.md)
- [Running Without Airflow](docs/guides/RUN_WITHOUT_AIRFLOW.md)
- [Docker Instructions](docs/guides/DOCKER_INSTRUCTIONS.md)
- [Troubleshooting](docs/troubleshooting/)
- [Developer Setup](docs/DEV_SETUP.md)

## Development

### Running Tests

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/test_pipeline.py
```

### Code Quality

```bash
# Linting
ruff check src/

# Type checking
mypy src/

# Formatting
black src/
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a pull request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---
**Version**: 5.0.0  
**Status**: âœ… Production Ready